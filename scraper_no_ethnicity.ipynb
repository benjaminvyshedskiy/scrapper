{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "810\n",
      "Data saved to variants_20000000-20002000.csv\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_row_data(html_row):\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_row, 'html.parser')\n",
    "\n",
    "    # Find the main row-layout div\n",
    "    row_div = soup.find(\"div\", class_=\"row-layout\")\n",
    "    # Extract relevant data from each div inside the row-layout\n",
    "    data = []\n",
    "    for div in row_div.find_all(\"div\", recursive=False):\n",
    "        text = ''.join(div.stripped_strings).replace('</d...', '').strip()\n",
    "        data.append(text)\n",
    "\n",
    "    # Ensure data is cleaned and aligned correctly\n",
    "    if len(data) < 9:\n",
    "        data.extend(['_'] * (9 - len(data)))  # Fill missing fields with '_'\n",
    "\n",
    "    # Create a dictionary to hold the final data\n",
    "    row_data = {\n",
    "        \"Variant ID\": data[0] if len(data) > 0 else '_',\n",
    "        \"Gene\": data[1] if len(data) > 1 else '_',\n",
    "        \"Consequence\": data[2] if len(data) > 2 else '_',\n",
    "        \"Variant Type\": data[3] if len(data) > 3 else '_',\n",
    "        \"ClinVar Significance\": data[4] if len(data) > 4 else '_',\n",
    "        \"Allele Count\": data[5] if len(data) > 5 else '_',\n",
    "        \"Allele Number\": data[6] if len(data) > 6 else '_',\n",
    "        \"Allele Frequency\": data[7] if len(data) > 7 else '_',\n",
    "        \"Homozygote Count\": data[8] if len(data) > 8 else '_'\n",
    "    }\n",
    "\n",
    "    return row_data\n",
    "\n",
    "def process_html_rows(html_rows):\n",
    "    # Process multiple rows and create a dataframe\n",
    "    processed_rows = [extract_row_data(row) for row in html_rows]\n",
    "    df = pd.DataFrame(processed_rows)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define the boundaries and the total number of rows\n",
    "boundaries = \"20000000-20002000\"\n",
    "total_rows = 810  # Total number of rows expected\n",
    "rows_per_scroll = 200  # Number of rows loaded per scroll\n",
    "\n",
    "# Set up the WebDriver with headless options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "#chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU acceleration\n",
    "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model (for Linux)\n",
    "chrome_options.add_argument(\"--window-size=1920x1080\")  # Set window size\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Navigate to the specific URL\n",
    "driver.get(f\"https://databrowser.researchallofus.org/variants/chr22:{boundaries}\")\n",
    "\n",
    "# Initialize a variable to store the HTML rows\n",
    "all_html_rows = set()  # Using a set to avoid duplicates\n",
    "\n",
    "try:\n",
    "    # Wait for the scrollable area to load\n",
    "    scroll_area = WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, \"scroll-area\"))\n",
    "    )\n",
    "    # Loop until the desired number of rows is loaded\n",
    "    while len(all_html_rows) < total_rows:\n",
    "        # Scroll to the bottom of the scrollable area\n",
    "        driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight\", scroll_area)\n",
    "        if len(all_html_rows)%1000 == 0:\n",
    "            print(len(all_html_rows))\n",
    "        # Wait for new data to load\n",
    "        #time.sleep(0.01)  # Adjust based on load time\n",
    "        \n",
    "        # Parse the page source after scrolling\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Extract rows with the class \"row-layout\"\n",
    "        rows = soup.find_all(\"div\", class_=\"row-layout\")\n",
    "\n",
    "        # Add only new rows to the set to avoid duplicates\n",
    "        all_html_rows.update(str(row) for row in rows)\n",
    "\n",
    "        # Stop if we have loaded the expected number of rows\n",
    "        if len(all_html_rows) >= total_rows:\n",
    "            break\n",
    "    \n",
    "    # Process the collected rows\n",
    "    if all_html_rows:\n",
    "        df = process_html_rows(list(all_html_rows))  # Convert the set back to a list\n",
    "        print(len(df))\n",
    "        \n",
    "        # Save the dataframe to an Excel file with the boundaries in the filename\n",
    "        filename = f\"variants_{boundaries}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Data saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No rows found in the page.\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
